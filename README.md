# DeeperNetworkComparison

Deep neural networks (DNNs) have, in recent years have dominated the state-of-the-art for image classification, reporting quality results and vastly outperforming shallower, more conventional Convolutional Neural Networks (CNNs). I compared 3 notable models from recent years - ResNet (2015), VGG (2015) and DenseNet (2017) in their effectiveness when trained and tested on the following common image-classification benchmark datasets: MNIST, Fashion-MNIST and CIFAR-10. MNIST is a digit-recognition task wherein models must classify hand-drawn digit samples between 1 and 9; a simple benchmark for models of this scope. Fashion-MNIST is an object classification task that requires models to label slightly more complex objects that have been highly normalised. CIFAR-10 is a more difficult object identification task that contains images with more noise and much more complex targets, some chosen specifically to share common visual features in order to increase task difficulty.

Subsequently, I analysed their training and testing accuracies for each experiment along with their testing average loss values, in order to determine potential strengths and weaknesses - diagnosing the reasoning for this. Additionally, trends and anomalies synchronous with DNN modelsâ€™ performance was identified and analysed.
