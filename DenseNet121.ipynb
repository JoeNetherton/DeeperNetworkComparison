{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet121.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlbJAoGWjyCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "#\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzM5BTFqwPa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set to true or false to enable/disable data augmentation\n",
        "DATA_AUGMENTATION = True\n",
        "\n",
        "#'cifar', 'fashion mnist', 'mnist'\n",
        "DATASET_USED = \"cifar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSBC0DXcjoXR",
        "colab_type": "code",
        "outputId": "97ee142b-5a83-4b30-c9f3-d3b59f2095b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "def resize(mnist):\n",
        "     #function resizes the dimensions of inputted dataset\n",
        "     data = []\n",
        "     for img in mnist:\n",
        "            resized_img = cv2.resize(img, (32, 32))\n",
        "            data.append(resized_img)\n",
        "     return data\n",
        "\n",
        "if DATASET_USED == \"cifar\":\n",
        "  # Load CIFAR10 Data\n",
        "  (X_train_val, y_train_val), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "if DATASET_USED == 'fashion_mnist':\n",
        "  (X_train_val, y_train_val), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "if DATASET_USED == 'mnist':\n",
        "  (X_train_val, y_train_val), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print('train images:', X_train_val.shape) # no.of samples * height * width * channels\n",
        "print('test images:', X_test.shape) # no.of samples * height * width * channels\n",
        "\n",
        "# convert labels to onehot-encoding \n",
        "y_train_val = tf.keras.utils.to_categorical(y_train_val, num_classes = 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes = 10)\n",
        "print('train labels:', y_train_val.shape) # no.of samples * num_classes\n",
        "print('test labels:', y_test.shape) # no.of samples * num_classes"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train images: (50000, 32, 32, 3)\n",
            "test images: (10000, 32, 32, 3)\n",
            "train labels: (50000, 10)\n",
            "test labels: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSY6EiRbjuda",
        "colab_type": "code",
        "outputId": "a127e170-13db-4be1-8a44-fb67781e2f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.1)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45000, 32, 32, 3), (5000, 32, 32, 3), (45000, 10), (5000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq33_apwj_AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bn_relu_convolution(x, nb_channels, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Creates a convolution layers consisting of BN-ReLU-Conv.\n",
        "    Optional: bottleneck, dropout\n",
        "    \n",
        "    \"\"\"\n",
        "    # Bottleneck\n",
        "    if bottleneck:\n",
        "        bottleneckWidth = 4\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.Conv2D(nb_channels * bottleneckWidth, (1, 1),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
        "        # Dropout\n",
        "        if dropout_rate:\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # BN-ReLU-Conv\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(nb_channels, (3, 3), padding='same')(x)\n",
        "\n",
        "    # Dropout\n",
        "    if dropout_rate:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZLTuGlVkGE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bn_relu_transition(x, nb_channels, dropout_rate=None, compression=1.0, weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Creates a transition layer between dense blocks as transition, which do convolution and pooling.\n",
        "    Works as downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu',)(x)\n",
        "    x = layers.Convolution2D(int(nb_channels * compression), (1, 1), padding='same',\n",
        "                             kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
        "\n",
        "    # Adding dropout\n",
        "    if dropout_rate:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXYW9-uvkIFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_block(x, num_layers, nb_channels, growth_rate, dropout_rate=None, bottleneck=False,\n",
        "                    weight_decay=1e-4):\n",
        "    \"\"\"\n",
        "    Creates a dense block and concatenates inputs\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        cb = bn_relu_convolution(x, growth_rate, dropout_rate, \n",
        "                                 bottleneck) # 1 conv if bottleneck = 0 else 2 conv if bottleneck = 1\n",
        "        nb_channels += growth_rate\n",
        "        x = layers.concatenate([cb, x])\n",
        "    return x, nb_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNFfT2IPkLAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def DenseNet(input_shape, dense_blocks, dense_layers, growth_rate, compression, bottleneck, \n",
        "                     weight_decay, dropout_rate, num_classes, ):\n",
        "        \"\"\"\n",
        "        Build the model\n",
        "        Returns: tf Keras Model instance\n",
        "        \"\"\"\n",
        "\n",
        "        print('Creating DenseNet with Bottleneck = {}'.format(bottleneck))\n",
        "        print('#############################################')\n",
        "        print('No.of. dense blocks: %s' % dense_blocks)\n",
        "        print('Layers per dense block: %s' % dense_layers)\n",
        "        print('#############################################')\n",
        "\n",
        "        # Input Layer\n",
        "        img_input = layers.Input(shape=input_shape, name = 'img_input')\n",
        "        nb_channels = growth_rate\n",
        "\n",
        "        # Input-convolution layer\n",
        "        x = layers.Conv2D(2 * growth_rate, (3, 3), padding='same', strides=(1, 1),name='input_conv', \n",
        "                          kernel_regularizer= tf.keras.regularizers.l2(weight_decay))(img_input)\n",
        "\n",
        "        # Building dense blocks\n",
        "        for block in range(dense_blocks - 1):\n",
        "            # Add dense_block\n",
        "            x, nb_channels = dense_block(x, dense_layers[block], nb_channels, growth_rate,\n",
        "                                     dropout_rate, bottleneck, weight_decay) \n",
        "\n",
        "            # Add transition\n",
        "            x = bn_relu_transition(x, nb_channels, dropout_rate, compression, weight_decay) # 1 conv layer\n",
        "            nb_channels = int(nb_channels * compression)\n",
        "\n",
        "        # Add last dense block without transition but with only global average pooling\n",
        "        x, nb_channels = dense_block(x, dense_layers[-1], nb_channels,\n",
        "                                          growth_rate, dropout_rate, weight_decay)\n",
        "        \n",
        "        # prediction of class happens here\n",
        "        x = layers.BatchNormalization(name = 'prediction_bn')(x)\n",
        "        x = layers.Activation('relu',  name = 'prediction_relu', )(x)\n",
        "        x = layers.GlobalAveragePooling2D( name = 'prediction_pool', )(x)\n",
        "        prediction = layers.Dense(num_classes, name = 'prediction_dense', activation='softmax')(x)\n",
        "\n",
        "        return tf.keras.Model(inputs=img_input, outputs=prediction, name='densenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2lzfRsskODa",
        "colab_type": "code",
        "outputId": "a3a90f2b-4538-4644-83b3-2ee03cc8d3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "dense_net = DenseNet(input_shape = (32,32,3), dense_blocks = 3, dense_layers = [16]*3,\n",
        "                     growth_rate = 12, compression = 0.5, num_classes = 10, bottleneck = True, \n",
        "                     dropout_rate = None, weight_decay = 1e-5)\n",
        "# dense_net.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating DenseNet with Bottleneck = True\n",
            "#############################################\n",
            "No.of. dense blocks: 3\n",
            "Layers per dense block: [16, 16, 16]\n",
            "#############################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25x8zqwtkd7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseNet(object):\n",
        "    \n",
        "    def __init__(self,input_shape=None, dense_blocks=3, dense_layers=-1, growth_rate=12, num_classes=None,\n",
        "                 dropout_rate=None, bottleneck=False, compression=1.0, weight_decay=1e-4, depth=40):\n",
        "        \n",
        "        # Parameters Check\n",
        "        if num_classes == None:\n",
        "            raise Exception(\n",
        "                'Please define number of classes (e.g. num_classes=10). This is required to create .')\n",
        "\n",
        "        if compression <= 0.0 or compression > 1.0:\n",
        "            raise Exception('Compression have to be a value between 0.0 and 1.0.')\n",
        "\n",
        "        if type(dense_layers) is list:\n",
        "            if len(dense_layers) != dense_blocks:\n",
        "                raise AssertionError('Number of dense blocks have to be same length to specified layers')\n",
        "        elif dense_layers == -1:\n",
        "            dense_layers = int((depth - 4) / 3)\n",
        "            if bottleneck:\n",
        "                dense_layers = int(dense_layers / 2)\n",
        "            dense_layers = [dense_layers for _ in range(dense_blocks)]\n",
        "        else:\n",
        "            dense_layers = [dense_layers for _ in range(dense_blocks)]\n",
        "\n",
        "        self.dense_blocks = dense_blocks\n",
        "        self.dense_layers = dense_layers\n",
        "        self.input_shape = input_shape\n",
        "        self.growth_rate = growth_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.bottleneck = bottleneck\n",
        "        self.compression = compression\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        \n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the model\n",
        "        Returns: tf Keras Model instance\n",
        "        \"\"\"\n",
        "        if self.bottleneck:\n",
        "            print('Creating DenseNet with Bottlenecks')\n",
        "        else:\n",
        "            print('Creating DenseNet without Bottlenecks')\n",
        "        print('-' * 50)\n",
        "        print('No.of. dense blocks: %s' % self.dense_blocks)\n",
        "        print('Layers per dense block: %s' % self.dense_layers)\n",
        "        print('-'* 50)\n",
        "\n",
        "        # Input Layer\n",
        "        img_input = layers.Input(shape = self.input_shape, name = 'img_input')\n",
        "        nb_channels = self.growth_rate\n",
        "\n",
        "        # Input-convolution layer\n",
        "        x = layers.Conv2D(2 * self.growth_rate, (3, 3), padding='same', strides=(1, 1),name='input_conv', \n",
        "                          kernel_regularizer= tf.keras.regularizers.l2(self.weight_decay))(img_input)\n",
        "\n",
        "        # Building dense blocks\n",
        "        for block in range(self.dense_blocks - 1):\n",
        "            # Add dense_block\n",
        "            x, nb_channels = self.dense_block(x, self.dense_layers[block], nb_channels, self.growth_rate,\n",
        "                                      self.dropout_rate, self.bottleneck, self.weight_decay) \n",
        "\n",
        "            # Add transition\n",
        "            x = self.bn_relu_transition(x, nb_channels, self.dropout_rate, \n",
        "                                        self.compression, self.weight_decay) # 1 conv layer\n",
        "            nb_channels = int(nb_channels * self.compression)\n",
        "\n",
        "        # Add last dense block without transition but with only global average pooling\n",
        "        x, nb_channels = self.dense_block(x, self.dense_layers[-1], nb_channels,\n",
        "                                          self.growth_rate, self.dropout_rate, self.weight_decay)\n",
        "        \n",
        "        # prediction of class happens here\n",
        "        x = layers.BatchNormalization(name = 'prediction_bn')(x)\n",
        "        x = layers.Activation('relu',  name = 'prediction_relu', )(x)\n",
        "        x = layers.GlobalAveragePooling2D( name = 'prediction_pool', )(x)\n",
        "        prediction = layers.Dense(self.num_classes, name = 'prediction_dense', activation='softmax')(x)\n",
        "\n",
        "        return tf.keras.Model(inputs=img_input, outputs=prediction, name='DenseNet')\n",
        "        \n",
        "        \n",
        "    def dense_block(self, x, num_layers, nb_channels, growth_rate, dropout_rate=None, bottleneck=False,\n",
        "                    weight_decay=1e-4):\n",
        "        \"\"\"\n",
        "        Creates a dense block and concatenates inputs\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            cb = self.bn_relu_convolution(x, growth_rate, dropout_rate, \n",
        "                                     bottleneck) # 1 conv if bottleneck = 0 else 2 conv if bottleneck = 1\n",
        "            nb_channels += growth_rate\n",
        "            x = layers.concatenate([cb, x])\n",
        "        return x, nb_channels\n",
        "\n",
        "        \n",
        "    def bn_relu_convolution(self, x, nb_channels, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
        "        \"\"\"\n",
        "        Creates a convolution layers consisting of BN-ReLU-Conv.\n",
        "        Optional: bottleneck, dropout\n",
        "\n",
        "        \"\"\"\n",
        "        # Bottleneck\n",
        "        if bottleneck:\n",
        "            bottleneckWidth = 4\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.Activation('relu')(x)\n",
        "            x = layers.Conv2D(nb_channels * bottleneckWidth, (1, 1),\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
        "            # Dropout\n",
        "            if dropout_rate:\n",
        "                x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "        # BN-ReLU-Conv\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.Conv2D(nb_channels, (3, 3), padding='same')(x)\n",
        "\n",
        "        # Dropout\n",
        "        if dropout_rate:\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def bn_relu_transition(self, x, nb_channels, dropout_rate=None, compression=1.0, weight_decay=1e-4):\n",
        "        \"\"\"\n",
        "        Creates a transition layer between dense blocks as transition, which do convolution and pooling.\n",
        "        Works as downsampling.\n",
        "        \"\"\"\n",
        "\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu',)(x)\n",
        "        x = layers.Convolution2D(int(nb_channels * compression), (1, 1), padding='same',\n",
        "                                 kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
        "\n",
        "        # Adding dropout\n",
        "        if dropout_rate:\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "        x = layers.AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_okkV2_okobY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dense_net.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(),\n",
        "                  metrics=['accuracy'],)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZQxLukrksLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, \n",
        "                                   horizontal_flip=True, vertical_flip=True)\n",
        "train_datagen.fit(X_train)\n",
        "train_data = train_datagen.flow(X_train, y_train, batch_size = 126)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV36cMq6kuIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen.fit(X_val)\n",
        "val_data = val_datagen.flow(X_val, y_val, batch_size = 126)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_EyYE7-kwyA",
        "colab_type": "code",
        "outputId": "6843212e-de19-42c2-b35c-f340e225c426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "# fits the model on batches with real-time data augmentation:\n",
        "\n",
        "if DATA_AUGMENTATION == True:\n",
        "  res_history = dense_net.fit(train_data, epochs = 100,\n",
        "                              validation_data = (X_val/255.,y_val), steps_per_epoch = 397)\n",
        "else:\n",
        "  res_history = dense_net.fit(X_train, y_train, epochs = 100,\n",
        "                                      validation_data = (X_val,y_val), steps_per_epoch = 397)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "397/397 [==============================] - 81s 205ms/step - loss: 1.7790 - accuracy: 0.4672 - val_loss: 2.3908 - val_accuracy: 0.3164\n",
            "Epoch 2/100\n",
            "397/397 [==============================] - 79s 198ms/step - loss: 1.3349 - accuracy: 0.6144 - val_loss: 1.4809 - val_accuracy: 0.5538\n",
            "Epoch 3/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 1.1364 - accuracy: 0.6720 - val_loss: 1.4825 - val_accuracy: 0.5962\n",
            "Epoch 4/100\n",
            "397/397 [==============================] - 78s 198ms/step - loss: 1.0154 - accuracy: 0.7077 - val_loss: 1.2718 - val_accuracy: 0.6202\n",
            "Epoch 5/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.9256 - accuracy: 0.7344 - val_loss: 1.2772 - val_accuracy: 0.6432\n",
            "Epoch 6/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.8525 - accuracy: 0.7561 - val_loss: 0.9371 - val_accuracy: 0.7288\n",
            "Epoch 7/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.7999 - accuracy: 0.7746 - val_loss: 0.9085 - val_accuracy: 0.7438\n",
            "Epoch 8/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.7476 - accuracy: 0.7899 - val_loss: 0.9265 - val_accuracy: 0.7404\n",
            "Epoch 9/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.7123 - accuracy: 0.7993 - val_loss: 1.1059 - val_accuracy: 0.6868\n",
            "Epoch 10/100\n",
            "397/397 [==============================] - 78s 196ms/step - loss: 0.6888 - accuracy: 0.8069 - val_loss: 0.8871 - val_accuracy: 0.7564\n",
            "Epoch 11/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.6579 - accuracy: 0.8164 - val_loss: 1.3105 - val_accuracy: 0.6400\n",
            "Epoch 12/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.6259 - accuracy: 0.8256 - val_loss: 0.8538 - val_accuracy: 0.7568\n",
            "Epoch 13/100\n",
            "397/397 [==============================] - 78s 197ms/step - loss: 0.6106 - accuracy: 0.8321 - val_loss: 0.7749 - val_accuracy: 0.7826\n",
            "Epoch 14/100\n",
            "397/397 [==============================] - 79s 199ms/step - loss: 0.5905 - accuracy: 0.8373 - val_loss: 1.0503 - val_accuracy: 0.7124\n",
            "Epoch 15/100\n",
            "397/397 [==============================] - 79s 199ms/step - loss: 0.5767 - accuracy: 0.8424 - val_loss: 0.8989 - val_accuracy: 0.7422\n",
            "Epoch 16/100\n",
            "397/397 [==============================] - 79s 198ms/step - loss: 0.5528 - accuracy: 0.8484 - val_loss: 0.7714 - val_accuracy: 0.7800\n",
            "Epoch 17/100\n",
            "397/397 [==============================] - 79s 199ms/step - loss: 0.5408 - accuracy: 0.8534 - val_loss: 0.8561 - val_accuracy: 0.7614\n",
            "Epoch 18/100\n",
            "397/397 [==============================] - 79s 198ms/step - loss: 0.5270 - accuracy: 0.8577 - val_loss: 0.7129 - val_accuracy: 0.7966\n",
            "Epoch 19/100\n",
            "397/397 [==============================] - 79s 198ms/step - loss: 0.5131 - accuracy: 0.8619 - val_loss: 1.0335 - val_accuracy: 0.7374\n",
            "Epoch 20/100\n",
            "397/397 [==============================] - 79s 198ms/step - loss: 0.5057 - accuracy: 0.8643 - val_loss: 0.7780 - val_accuracy: 0.7914\n",
            "Epoch 21/100\n",
            " 84/397 [=====>........................] - ETA: 58s - loss: 0.4903 - accuracy: 0.8722"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXF7X9wRlKxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = res_history.history\n",
        "print(history_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSymHnrHE1j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = res_history.history['accuracy']\n",
        "val_acc = res_history.history['val_accuracy']\n",
        "loss = res_history.history['loss']\n",
        "val_loss = res_history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') # \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') # b is for \"solid blue line\"\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HZtUgkqE3qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.clf() # clear figure\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng_OLxzzpB8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "nb_classes = 10\n",
        "\n",
        "# Initialize the prediction and label lists(tensors)\n",
        "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(test_loader):\n",
        "        inputs = inputs.cuda()\n",
        "        #classes = classes.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Append batch prediction results\n",
        "        predlist=torch.cat([predlist,preds.view(-1).cpu().detach()])\n",
        "        lbllist=torch.cat([lbllist,classes.view(-1).cpu().detach()])\n",
        "\n",
        "# Confusion matrix\n",
        "lbllist= lbllist.numpy()\n",
        "predlist= predlist.numpy()\n",
        "\n",
        "print(\"confusion_matrix:\")\n",
        "conf_mat=confusion_matrix(lbllist, predlist)\n",
        "print(conf_mat)\n",
        "print('\\n')\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"per-class accuracy:\")\n",
        "class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "print(class_accuracy)\n",
        "print('\\n')\n",
        "print(\"prf:\")\n",
        "\n",
        "#PRF\n",
        "prf = precision_recall_fscore_support(lbllist, predlist)\n",
        "print(\"class precision:\", prf[0], '\\n')\n",
        "print(\"class recall:\", prf[1], '\\n')\n",
        "print(\"class f1-score:\", prf[2], '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtD5vTs-fVxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}